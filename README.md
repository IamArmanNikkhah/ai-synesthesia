# ai-synesthesia

**aiâ€‘synesthesia** is a research project that explores multimodal learning models capable of translating between audio and visual modalitiesâ€”teaching machines to "see" sounds and "hear" visuals. Inspired by the phenomenon of synesthesia in humans, this repository contains code to train, evaluate, and visualize models that map audio signals to images and vice versa.

## Features

- ðŸŽµâ†’ðŸŽ¨ **Audioâ€‘toâ€‘Image Translation** â€“ Generate images conditioned on audio features such as pitch, tempo, and timbre.
- ðŸŽ¨â†’ðŸŽµ **Imageâ€‘toâ€‘Audio Generation** â€“ Synthesize audio sequences based on visual inputs like color palettes, textures, or shapes.
- ðŸ¤¬ **Multimodal Encoderâ€“Decoder Architecture** â€“ Flexible neural network framework that can plug in different backbones (CNNs, RNNs, Transformers) for encoding audio spectrograms and decoding visuals, or vice versa.
- ðŸ”¬ **Evaluation Toolkit** â€“ Scripts for quantitative metrics (e.g. FID, spectral coherence) and qualitative visualization of crossâ€‘modal translations.
- ðŸ“Š **Training Pipeline** â€“ Endâ€‘toâ€‘end training scripts with configurable hyperparameters, checkpointing, and support for GPU acceleration.

## Project Structure

- `model/` â€“ Model definitions and architectures for encoders/decoders.
- `training/` â€“ Training loops, dataset loaders, configuration files.
- `evaluation/` â€“ Evaluation scripts and metrics for both modalities.
- `utils/` â€“ Utility functions for preprocessing, logging, and visualization.
- `pipeline_full.py` â€“ Example script to run the full synesthesia pipeline from data loading through training to evaluation.

## Getting Started

1. **Clone the repository**

   ```bash
   git clone https://github.com/IamArmanNikkhah/ai-synesthesia.git
   cd ai-synesthesia
   ```

2. **Install dependencies**

   It's recommended to use Python 3.9+ with `pip` and virtual environment:

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. **Prepare the dataset**

   - Place your paired audioâ€“image dataset in a directory structure similar to:

     ```
     data/
       train/
         audio/
         images/
       val/
         audio/
         images/
     ```
   - Supported audio formats: `.wav`, `.mp3`. Images should be `.png` or `.jpg`.

4. **Train a model**

   To train the default audioâ€‘toâ€‘image model:

   ```bash
   python training/train.py \
       --config configs/audio2img.yaml \
       --data_dir data/train \
       --val_dir data/val \
       --epochs 100 \
       --batch_size 16
   ```

   For imageâ€‘toâ€‘audio, use the corresponding config file (`configs/img2audio.yaml`).

5. **Evaluate and visualize**

   ```bash
   python evaluation/evaluate_model.py --checkpoint path/to/checkpoint.pth --output_dir outputs
   ```

   Generated images and audio will be saved in the `outputs` directory for inspection.

## Contributing

Contributions are welcome! If you have ideas to improve the architecture, add new datasets, or implement additional crossâ€‘modal tasks:

- Fork the repository and create a new branch: `git checkout -b feature/your-feature`.
- Commit your changes and push your branch.
- Open a pull request describing your contributions.

Please follow standard Python coding conventions and document any new modules.

## License

This project is licensed under the MIT License. See the `LICENSE` file for details.
